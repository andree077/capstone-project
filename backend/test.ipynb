{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_lib():\n",
    "  import whisper\n",
    "  import datetime\n",
    "  import subprocess\n",
    "  import torch\n",
    "  import pyannote.audio\n",
    "\n",
    "  #speaker vefification - pretrained model -->  identifies speakers\n",
    "  from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
    "  embedding_model = PretrainedSpeakerEmbedding(\n",
    "      \"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "      device=torch.device(\"cuda\"))\n",
    "\n",
    "  from pyannote.audio import Audio\n",
    "  from pyannote.core import Segment\n",
    "\n",
    "  import wave\n",
    "  import contextlib\n",
    "\n",
    "  #cluster\n",
    "  from sklearn.cluster import AgglomerativeClustering\n",
    "  import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarize_call(call_rec):\n",
    "\n",
    "  #import necessary libraries\n",
    "  import whisper\n",
    "  import datetime\n",
    "  import subprocess\n",
    "  import torch\n",
    "  import pyannote.audio\n",
    "  from pydub import AudioSegment\n",
    "\n",
    "  #speaker vefification - pretrained model -->  identifies speakers\n",
    "  from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
    "  embedding_model = PretrainedSpeakerEmbedding(\n",
    "      \"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "      device=torch.device(\"cuda\"))\n",
    "\n",
    "  from pyannote.audio import Audio\n",
    "  from pyannote.core import Segment\n",
    "\n",
    "  import wave\n",
    "  import contextlib\n",
    "\n",
    "  #cluster\n",
    "  from sklearn.cluster import AgglomerativeClustering\n",
    "  import numpy as np\n",
    "\n",
    "\n",
    "  #initialise values\n",
    "  num_speakers = 2\n",
    "\n",
    "  language = 'English'\n",
    "\n",
    "  model_size = 'medium'\n",
    "\n",
    "\n",
    "  model_name = model_size\n",
    "  if language == 'English' and model_size != 'large':\n",
    "    model_name += '.en'\n",
    "\n",
    "  #check if code isnt wav file\n",
    "  if call_rec[-3:] != 'wav':\n",
    "    subprocess.call(['ffmpeg', '-i', call_rec, 'audio.wav', '-y'])\n",
    "    path = 'audio.wav'\n",
    "\n",
    "  #loading the model\n",
    "  model = whisper.load_model(model_size)\n",
    "\n",
    "  result = model.transcribe(call_rec)\n",
    "  segments = result[\"segments\"]\n",
    "\n",
    "  audio = AudioSegment.from_file(call_rec)\n",
    "\n",
    "  # Get the raw audio data as an array of samples\n",
    "  samples = audio.get_array_of_samples()\n",
    "  frames = len(samples)\n",
    "\n",
    "  # Get the sample rate (frame rate) of the audio\n",
    "  rate = audio.frame_rate\n",
    "\n",
    "  # Print the results\n",
    "  print(\"Frames:\", len(samples))\n",
    "  print(\"Frame Rate:\", rate)\n",
    "\n",
    "  duration = frames/float(rate)\n",
    "\n",
    "  audio = Audio()\n",
    "\n",
    "  def segment_embedding(segment):\n",
    "    start = segment[\"start\"]\n",
    "\n",
    "    # Whisper overshoots the end timestamp in the last segment\n",
    "    end = min(duration, segment[\"end\"])\n",
    "    clip = Segment(start, end)\n",
    "    waveform, sample_rate = audio.crop(path, clip)\n",
    "    return embedding_model(waveform[None])\n",
    "\n",
    "  embeddings = np.zeros(shape=(len(segments), 192))\n",
    "\n",
    "  for i, segment in enumerate(segments):\n",
    "    embeddings[i] = segment_embedding(segment)\n",
    "\n",
    "  embeddings = np.nan_to_num(embeddings)\n",
    "\n",
    "  clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
    "  labels = clustering.labels_\n",
    "  for i in range(len(segments)):\n",
    "    segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
    "\n",
    "  return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_linguistic(sentence):\n",
    "  import os\n",
    "  import pandas as pd\n",
    "  from gensim.models import Word2Vec\n",
    "  import nltk\n",
    "  from nltk.tokenize import word_tokenize\n",
    "  import string\n",
    "  import numpy as np\n",
    "\n",
    "  model_path = \"C:/Projects/capstone-project/backend/word2vec/wordtovector.model\"\n",
    "  model = Word2Vec.load(model_path)\n",
    "  tokens = word_tokenize(sentence)\n",
    "  vectors = [model.wv[word] for word in tokens if word in model.wv.key_to_index]\n",
    "  if vectors:\n",
    "    return np.mean(vectors, axis=0)\n",
    "  return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_acoustic(path,start,end):\n",
    "  import librosa\n",
    "  import os\n",
    "  import shutil\n",
    "  import numpy as np\n",
    "  from sklearn.cluster import KMeans\n",
    "  from pydub import AudioSegment\n",
    "  start = int(start*1000)\n",
    "  end = int(end*1000)\n",
    "  audio, sr = librosa.load(path, sr=None)\n",
    "  audio_file = audio[start:end]\n",
    "  # extract the features\n",
    "  neutral_mfccs = librosa.feature.mfcc(y=audio_file, sr=sr, n_mfcc=30)\n",
    "  neutral_chroma = librosa.feature.chroma_stft(y=audio_file, sr=sr, n_chroma=20)\n",
    "\n",
    "  # concatenate the features into a single feature vector\n",
    "  features = np.concatenate((neutral_mfccs.mean(axis=1), neutral_mfccs.var(axis=1), neutral_chroma.mean(axis=1), neutral_chroma.var(axis=1)))\n",
    "\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(segment, path):\n",
    "  import numpy as np\n",
    "\n",
    "  ling_features = extract_linguistic(segment['text'])\n",
    "  print(ling_features)\n",
    "  print(len(ling_features))\n",
    "\n",
    "  acoustic_features = extract_acoustic(path, segment['start'], segment['end'])\n",
    "  print(acoustic_features)\n",
    "  print(len(acoustic_features))\n",
    "\n",
    "  #Combining the features\n",
    "  ling_features = np.array(ling_features)\n",
    "  acoustic_features = np.array(acoustic_features)\n",
    "\n",
    "  combined_features = np.concatenate((ling_features,acoustic_features))\n",
    "  print(len(combined_features))\n",
    "\n",
    "  return combined_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "  from tensorflow.keras.models import load_model\n",
    "  import tensorflow as tf\n",
    "\n",
    "  comms_round = 10\n",
    "  model_path = \"/content/drive/MyDrive/capstone/global_model/globalmodel.h5\"\n",
    "  loss='categorical_crossentropy'\n",
    "  metrics = ['accuracy',tf.keras.metrics.Recall(),tf.keras.metrics.Precision() ]\n",
    "  optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.00001, decay=1e-6)\n",
    "\n",
    "  model = load_model(model_path)\n",
    "\n",
    "  model.compile(loss=loss,\n",
    "                optimizer=optimizer,\n",
    "                metrics=metrics)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(features,model):\n",
    "  import numpy as np\n",
    "  class_labels = ['Neutral','Angry','Happy','Confused']\n",
    "  features = np.expand_dims(features, axis=0)\n",
    "  # Predict the output for the single input\n",
    "  logits = model.predict(features)\n",
    "\n",
    "  # Assuming the model has softmax activation for multiclass classification, you can convert logits to probabilities\n",
    "  probabilities = tf.nn.softmax(logits)\n",
    "\n",
    "  # Find the index of the class with the highest probability\n",
    "  predicted_class_index = np.argmax(probabilities)\n",
    "\n",
    "  # Get the corresponding class label\n",
    "  predicted_class_label = class_labels[predicted_class_index]\n",
    "\n",
    "  # Return the predicted probabilities\n",
    "  return predicted_class_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing():\n",
    "\n",
    "  import os\n",
    "  from google.colab import drive\n",
    "  from google.colab import files\n",
    "\n",
    "  output_directory = \"/content/drive/MyDrive/Capstone_Dataset/testing\"\n",
    "  os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "  '''\n",
    "  #inputing single audio file\n",
    "  audio = input_audio()\n",
    "\n",
    "  '''\n",
    "  #to input multiple audio files\n",
    "  audio=[]\n",
    "  opt = 'y'\n",
    "  while opt == 'y':\n",
    "    rec = files.upload()\n",
    "    uploaded_file_name = list(rec.keys())[0]\n",
    "    content = rec[uploaded_file_name]\n",
    "\n",
    "    # Specify the output file path\n",
    "    output_filename = os.path.join(output_directory, uploaded_file_name)\n",
    "\n",
    "    # Save the content to the specified file path\n",
    "    with open(output_filename, 'wb') as f:\n",
    "      f.write(content)\n",
    "\n",
    "    audio.append(output_filename)\n",
    "    opt = input(\"continue?\")\n",
    "\n",
    "\n",
    "  for call_rec in audio:\n",
    "    # diarize call_rec\n",
    "    call_data = diarize_call(call_rec)\n",
    "\n",
    "    print(call_data)\n",
    "\n",
    "    i=0\n",
    "\n",
    "    for segment in call_data:\n",
    "      #extract features\n",
    "      features = feature_extraction(segment, call_rec)\n",
    "\n",
    "      print(features)\n",
    "      print(len(features))\n",
    "\n",
    "      model = load_model()\n",
    "\n",
    "      emotion = predict_emotion(features,model)\n",
    "      segment['emotion'] = emotion\n",
    "\n",
    "  for i in segments:\n",
    "    print(i['text'],\"spoken by\", i['speaker'],\"is most likey\", i['emotion'] )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
